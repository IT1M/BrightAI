{
“site”: “https: //brightai.site”,
“timestamp”: “2026-02-01T00: 00: 00+03: 00”,
“analysis_settings”: {
“pages_analyzed”: “كل الصفحات”,
“device_emulation”: “mobile (محاكاة الشبكة والجهاز الافتراضية)”,
“tools_intended”: [“Chrome Lighthouse (mobile) - simulated throttling”, “robots.txt analysis”, “meta tags crawler”
        ]
    },
“summary”: {
“verdict”: “تقرير تقني مُفصّل وقابل للتطبيق. تم استبدال معالجة ملف robots.txt بملف إنتاجي مخصص (انظر recommended_robots_txt). بقية الأقسام تتضمن اكتشافات، درجات خطورة، وتوصيات فورية قابلة للتنفيذ.”,
“overall_severity_score”: 7,
“overall_recommendation_priority”: “عالي — نفّذ التعديلات على robots.txt وموارد العرض أولًا ثم تابع تحسين Core Web Vitals”
    },
“crawlability”: {
“notes”: “تحليل الزحف يشمل ملف robots.txt الحالي، إعدادات meta robots في الصفحة، ورابط الخريطة sitemap.xml.”,
“issues”: [
            {
“id”: “crawl-001”,
“title”: “تضارب وسياسات قديمة في robots.txt (تمت استبدالها)”,
“description”: “الملف السابق احتوى قواعد تمنع تحميل موارد CSS/JS واشتمل على توجيهات غير مدعومة مثل X-Robots-Tag و Host. هذا يعيق عملية الرندر لدى محركات البحث ويؤثر على Lighthouse.”,
“severity_label”: “عالي”,
“severity_score”: 9,
“immediate_fix”: “استبدال robots.txt بملف إنتاجي مُحسّن لا يمنع موارد العرض، ويمنع المسارات الحساسة فقط. (تم توفير ملف مقترح في الحقل recommended_robots_txt).”,
“notes_after_fix”: “بعد استبدال الملف، اطلب من Google إعادة جلب robots.txt من خلال Google Search Console ثم اطلب إعادة فهرسة للصفحات الحساسة.”
            },
            {
“id”: “crawl-002”,
“title”: “تعليمات meta robots غير موحّدة عبر الصفحات”,
“description”: “توقّع وجود صفحات بدون tag meta robots أو صفحات قد تحتوي على noindex عن طريق الخطأ (تحتاج فحص شامل لكل صفحة).”,
“severity_label”: “متوسط”,
“severity_score”: 6,
“immediate_fix”: “فحص جميع الصفحات وإضافة <meta name=\"robots\" content=\"index, follow\"> للصفحات العامة، واستخدام noindex فقط للصفحات الحساسة/اختبار.”,
“automation_task”: “استخدام crawler لفحص وجود meta robots عبر كل URL وتصدير لائحة الصفحات التي تحتوي noindex أو تفتقد الوسم.”
            },
            {
“id”: “crawl-003”,
“title”: “Sitemap: تحقق من تطابق sitemap.xml مع مسارات الموقع الحقيقية”,
“description”: “وجود رابط Sitemap في robots.txt جيد؛ تأكد أن sitemap.xml محدث ويحتوي فقط على صفحات مفيدة وبدون صفحات محظورة أو خطأ 4xx/5xx.”,
“severity_label”: “متوسط”,
“severity_score”: 5,
“immediate_fix”: “تحديث sitemap وإعادة إرساله في Google Search Console، وإزالة روابط لصفحات محظورة أو ميتة.”
            }
        ]
    },
“meta_tags_and_headings”: {
“notes”: “تحقق من وجود العلامات الأساسية (title, meta description) وتوافق هيكل العناوين H1..H6.”,
“issues”: [
            {
“id”: “meta-001”,
“title”: “صفحات بدون عناوين (title) فريدة أو مكررة”,
“description”: “من الممكن وجود صفحات تفتقد title أو تحتوي على عناوين مكررة أو قصيرة جداً مما يؤثر على CTR وفهرسة الكلمات المفتاحية.”,
“severity_label”: “عالي”,
“severity_score”: 8,
“immediate_fix”: “إنشاء عنوان فريد لكل صفحة بطول 50-60 حرفًا يتضمن الكلمات المفتاحية المستهدفة للسوق السعودي.”
            },
            {
“id”: “meta-002”,
“title”: “وصف ميتا (meta description) مفقود أو مكرر”,
“description”: “بعض الصفحات المحتملة تفتقد وصفًا موجزًا يساعد نتائج البحث على الظهور بشكل أفضل.”,
“severity_label”: “متوسط”,
“severity_score”: 6,
“immediate_fix”: “إضافة أوصاف ميتا فريدة بطول 120-160 حرفًا لكل صفحة رئيسية ومقالات المدونة.”
            },
            {
“id”: “meta-003”,
“title”: “هيكل العناوين (H1-H6) غير متناسق”,
“description”: “احتمال وجود أكثر من H1 في صفحة أو تسلسل غير صحيح (مثال: H1 ثم H3 مباشرة) يؤثر في فهم محركات البحث لموضوع الصفحة.”,
“severity_label”: “متوسط”,
“severity_score”: 5,
“immediate_fix”: “ضمان وجود H1 واحد لكل صفحة واستخدام تسلسل هرمي منطقي للعناوين (H2, H3 …). إجراء فحص آلي للعناوين في كل صفحة.”
            }
        ]
    },
“core_web_vitals”: {
“notes”: “القيم الحقيقية يجب استخراجها من Lighthouse / PageSpeed / Search Console. الأسفل توصيف فني وتوقعات وتأثير robots.txt السابق على النتائج.”,
“expected_impact_of_previous_robots”: “حظر موارد CSS/JS يؤدي إلى نتائج متدهورة خاصة في LCP وCLS وINP بسبب عدم القدرة على الرندر الصحيح.”,
“metrics_summary_template”: {
“per_page”: [
                {
“url”: “/”,
“LCP_ms”: null,
“INP_ms”: null,
“CLS”: null,
“FID_ms”: null,
“observations”: “قِس فعليًا باستخدام Lighthouse. إن كانت موارد محجوبة فستكون القيم أسوأ بكثير.”
                }
            ]
        },
“general_issues_and_recommendations”: [
            {
“id”: “vital-001”,
“title”: “LCP بطيء (>2.5s)”,
“possible_causes”: [“صور كبيرة غير مضغوطة”, “التحمّل البطيء للخادم”, “CSS يمنع العرض”
                ],
“severity_label”: “عالي”,
“severity_score”: 9,
“immediate_fix”: “ضغط الصور (WebP/AVIF)، تقديم صور بأبعاد مناسبة، تفعيل CDN وHTTP/2 أو HTTP/3، تقليل الوقت لرد الخادم، تأجيل تحميل CSS/JS غير الحرجة.”
            },
            {
“id”: “vital-002”,
“title”: “INP / FID مرتفع (>200ms)”,
“possible_causes”: [“جافاسكربت كثيف على الخيط الرئيسي (main thread)”, “تفاعل مبالغ به قبل تحميل الواجهات”
                ],
“severity_label”: “متوسط-عالي”,
“severity_score”: 7,
“immediate_fix”: “تقسيم الحزم (code-splitting)، تقليل كود الطرف الرئيسي، استخدام web workers، تحسين handlers للأحداث.”
            },
            {
“id”: “vital-003”,
“title”: “CLS مرتفع (>0.1)”,
“possible_causes”: [“صور بدون أبعاد/بدون reserving space”, “إعلانات أو iframes تُضاف بعد التحميل”
                ],
“severity_label”: “متوسط”,
“severity_score”: 6,
“immediate_fix”: “تحديد width/height للصور أو استخدام aspect-ratio، حجز مساحة للعناصر الديناميكية، التحميل الكسول للصور مع الحفاظ على إطار العنصر.”
            }
        ]
    },
“mobile_usability_and_responsiveness”: {
“issues”: [
            {
“id”: “mobile-001”,
“title”: “عناصر قابلة للنقر صغيرة أو متقاربة”,
“description”: “أزرار أو روابط قد تكون أصغر من معيار 48x48 CSS pixels أو متقاربة جدًا على الشاشات الصغيرة.”,
“severity_label”: “متوسط”,
“severity_score”: 6,
“immediate_fix”: “تكبير حجم عناصر النقر، زيادة المسافات البينية، اختبار على أجهزة فعلية ومحاكاة.”
            },
            {
“id”: “mobile-002”,
“title”: “نصوص صغيرة جدًا أو غير قابلة للقراءة”,
“severity_label”: “متوسط”,
“severity_score”: 5,
“immediate_fix”: “تأكيد استخدام نسب الخطوط المرنة (rem) وضبط قواعد media queries لتحسين قابلية القراءة.”
            },
            {
“id”: “mobile-003”,
“title”: “التخطيط غير مستجيب في بعض الشرائح”,
“severity_label”: “متوسط”,
“severity_score”: 6,
“immediate_fix”: “مراجعة CSS Grid/Flex عبر النوافذ الصغيرة، اختبار صفحات المهمة (الرئيسية، الخدمات، المقالات) وضبط نقاط التوقف (breakpoints).”
            }
        ]
    },
“spa_and_rendering_recommendation”: {
“detection”: “من خلال بنية الملفات المرسلة، الموقع يحتوي صفحات ثابتة لكن قد يستخدم مكونات/باندلات JS؛ لذا تقييم SPA محتمل.”,
“is_spa”: “محتمل/جزئي”,
“recommended_approach”: {
“preferred”: “Hybrid SSR / Prerender”,
“rationale”: [
“SSR (مثل Next.js/Nuxt) يعطي HTML جاهزاً للمحركات ويحسّن الفهرسة والأداء البدئي (LCP).”,
“Prerendering (Vite SSG، prerender.io) مناسب إذا كان المحتوى ثابتا أو صفحات المقالات لا تتغير كثيرًا.”,
“إذا كنت تعتمد كثيرًا على تفاعلات العميل (client-side routing) فاعتمد Hybrid: SSR للصفحات العامة + CSR للواجهات التفاعلية.”
            ],
“immediate_action”: “إنشاء خطة انتقال: اختبر prerender لصفحات المدونة والصفحات الهامة، وادمج SSR لصفحات البيع/الصفحة الرئيسية إن أمكن.”
        }
    },
“recommended_robots_txt”: {
“note”: “هذا ملف robots.txt الإنتاجي المُقترح ليحلّ محل الملف القديم — مناسب لموقع ثابت أو لمشروع SPA/SSR مستضاف على Firebase/Hosting/CDN.”,
“content”: “# ======================================================\n# BrightAI – Production Robots.txt (Static site / Firebase / Hosting)\n# Generated: 2026-02-01\n# ======================================================\n\nUser-agent: \nAllow: /\n\n# ––––––––––\n# Block sensitive paths (do not expose admin/config/backups)\n# ––––––––––\nDisallow: /admin/\nDisallow: /private/\nDisallow: /config/\nDisallow: /backup/\nDisallow: /backups/\nDisallow: /logs/\nDisallow: /tmp/\nDisallow: /cache/\nDisallow: /.git/\nDisallow: /.env\nDisallow: /.vscode/\nDisallow: /node_modules/\nDisallow: /tests/\nDisallow: /tests/\n\n# ––––––––––\n# Dev / build artifacts (common)\n# ––––––––––\nDisallow: /_next/\nDisallow: /.nuxt/\nDisallow: /dist/\nDisallow: /build/\nDisallow: /src/\n\n# ––––––––––\n# NEVER block rendering resources (CRITICAL for Lighthouse & Google)\n# ––––––––––\nAllow: /assets/\nAllow: /static/\nAllow: /images/\nAllow: /fonts/\nAllow: /css/\nAllow: /js/\nAllow: /favicon.ico\n\n# ––––––––––\n# Blog / content (ensure crawlers see content)\n# ––––––––––\nAllow: /blog/\nAllow: /blogger/\nAllow: /frontend/pages/\n\n# ––––––––––\n# API rules\n# ––––––––––\nDisallow: /api/auth/\nDisallow: /api/internal/\nAllow: /api/public/\n\n# ––––––––––\n# Files handling - block non-public dump files\n# ––––––––––\nDisallow: /.map$\nDisallow: /.log$\nDisallow: /.sql$\nDisallow: /.zip$\n\n# ––––––––––\n# SEO essential files\n# ––––––––––\nAllow: /sitemap.xml\nAllow: /robots.txt\nAllow: /manifest.json\nAllow: /schema.json\n\n# ––––––––––\n# Googlebot optimization\n# ––––––––––\nUser-agent: Googlebot\nAllow: /\nCrawl-delay: 1\n\n# ––––––––––\n# Sitemap\n# ––––––––––\nSitemap: https: //brightai.site/sitemap.xml”,
“severity_label”: “عالي (تصحيح حاسم)”,
“severity_score”: 10,
“apply_instructions”: [
“ضع الملف في جذر الاستضافة العامة (مثلاً public/ عند استخدام Firebase Hosting).”,
“بعد النشر، تحقق من https: //brightai.site/robots.txt وأن المحتوى مطابق.”,
“في Google Search Console استخدم أداة اختبار robots.txt وأعد إرسال sitemap.”
        ]
    },
“immediate_actions_checklist”: [
        {
“task”: “استبدال robots.txt بالنسخة الموصى بها”,
“priority”: “عالي”,
“estimated_effort”: “5-15 دقيقة للنشر و5 دقائق لإعادة التحقق في GSC”
        },
        {
“task”: “فحص/meta robots عبر كل الصفحات (crawler)”,
“priority”: “عالي”,
“estimated_effort”: “30-60 دقيقة لإجراء الزحف الكامل”
        },
        {
“task”: “إضافة/تحسين title و meta descriptions للصفحات الرئيسية والمدونة”,
“priority”: “عالي”,
“estimated_effort”: “لكل صفحة 5-15 دقيقة”
        },
        {
“task”: “تشغيل Lighthouse (mobile) على الصفحات الرئيسية بعد نشر robots الجديد”,
“priority”: “عالي”,
“estimated_effort”: “لكل صفحة 3-5 دقائق”
        },
        {
“task”: “تحسين صور LCP وضغط الأصول، تفعيل CDN”,
“priority”: “متوسط-عالي”,
“estimated_effort”: “ساعتان إلى عدة أيام حسب حجم الموقع”
        }
    ],
“notes_and_next_steps”: {
“note_1”: “التقرير هنا مُهيأ ليكون عمليًا وقابلًا للتنفيذ فورًا. بعض القياسات العددية الدقيقة (قيم LCP, INP, CLS) تحتاج تشغيل Lighthouse فعلي بعد نشر robots الجديد للحصول على بيانات حقيقية.”,
“note_2”: “إن رغبت، أستطيع الآن: (أ) دمج robots-final.txt داخل أرشيف المشروع وإعادة توفير zip جاهز للنشر، (ب) تشغيل فحص آلي للـ meta tags داخل ملفات المشروع المرفوعة، (ج) توليد قائمة URLs ذات أولوية لإعادة الفهرسة.”,
“preferred_language”: “العربية”
    }
}